import torch
import triton
import triton.language as tl
import time

def print_device_info():
    device = torch.cuda.current_device()
    props = torch.cuda.get_device_properties(device)
    print(f"Device: {props.name}")
    print(f"Compute Capability: {props.major}.{props.minor}")

def benchmark(fn, *args, warmup=10, rep=100):
    # Warmup (GPU lazy init, cache)
    for _ in range(warmup):
        fn(*args)
    torch.cuda.synchronize()

    start = time.time()
    for _ in range(rep):
        fn(*args)
    torch.cuda.synchronize()
    end = time.time()

    avg_ms = (end - start) * 1000 / rep
    return avg_ms

def get_autotune_config():
    return [
        # Ultra extreme split-K for small problems
        triton.Config({'BLOCK_M': 16, 'BLOCK_N': 16, 'BLOCK_K': 16, 'SPLIT_K': 32}, num_stages=1, num_warps=1),
        triton.Config({'BLOCK_M': 16, 'BLOCK_N': 16, 'BLOCK_K': 16, 'SPLIT_K': 32}, num_stages=1, num_warps=1),
        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 16, 'BLOCK_K': 16, 'SPLIT_K': 32}, num_stages=1, num_warps=1),
        
        # Extreme split-K for medium problems  
        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 16, 'BLOCK_K': 16, 'SPLIT_K': 16}, num_stages=2, num_warps=2),
        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 16, 'BLOCK_K': 16, 'SPLIT_K': 16}, num_stages=2, num_warps=2),
        triton.Config({'BLOCK_M': 16, 'BLOCK_N': 16, 'BLOCK_K': 32, 'SPLIT_K': 16}, num_stages=2, num_warps=1),
        
        # Heavy split-K for 8k single
        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 16, 'BLOCK_K': 16, 'SPLIT_K': 8}, num_stages=3, num_warps=2),
        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 16, 'BLOCK_K': 32, 'SPLIT_K': 8}, num_stages=3, num_warps=2),
        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 16, 'BLOCK_K': 16, 'SPLIT_K': 8}, num_stages=3, num_warps=4),
        
        # Optimized for multi-group
        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 16, 'BLOCK_K': 32, 'SPLIT_K': 4}, num_stages=3, num_warps=4),
        triton.Config({'BLOCK_M': 256, 'BLOCK_N': 16, 'BLOCK_K': 32, 'SPLIT_K': 2}, num_stages=3, num_warps=8),
        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 16, 'BLOCK_K': 64, 'SPLIT_K': 4}, num_stages=3, num_warps=2),
        
        # Large blocks for biggest problems
        triton.Config({'BLOCK_M': 256, 'BLOCK_N': 16, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=3, num_warps=8),
        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 16, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=3, num_warps=4),
        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 32, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=3, num_warps=4),
    ]

@triton.autotune(
    configs=get_autotune_config(),
    key=['M', 'N', 'K'],
)
@triton.jit
def grouped_gemm_kernel_optimized(
    A_ptr, B_ptr, C_ptr,
    M, N, K, GROUPS,
    stride_ag, stride_am, stride_ak,
    stride_bg, stride_bk, stride_bn,
    stride_cg, stride_cm, stride_cn,
    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,
    SPLIT_K: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    num_pid_m = tl.cdiv(M, BLOCK_M)
    num_pid_n = tl.cdiv(N, BLOCK_N)
    num_pid_k = SPLIT_K
    
    num_pid_in_group = num_pid_m * num_pid_n * num_pid_k
    group_id = pid // num_pid_in_group
    
    if group_id >= GROUPS:
        return
        
    pid_in_group = pid % num_pid_in_group
    pid_k = pid_in_group % num_pid_k
    pid_mn = pid_in_group // num_pid_k
    
    # Swizzle for cache optimization
    GROUP_M = 8
    pid_m = (pid_mn % GROUP_M) * (num_pid_m // GROUP_M) + (pid_mn // GROUP_M // num_pid_n)
    pid_n = (pid_mn // GROUP_M) % num_pid_n
    
    if SPLIT_K > 1:
        k_start = pid_k * tl.cdiv(K, SPLIT_K)
        k_end = tl.minimum(k_start + tl.cdiv(K, SPLIT_K), K)
    else:
        k_start = 0
        k_end = K
    
    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    offs_k = k_start + tl.arange(0, BLOCK_K)
    
    a_base = A_ptr + group_id * stride_ag + offs_m[:, None] * stride_am
    b_base = B_ptr + group_id * stride_bg + offs_n[None, :] * stride_bn
    
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)
    
    for k in range(k_start, k_end, BLOCK_K):
        a_mask = (offs_m[:, None] < M) & (offs_k[None, :] < K)
        b_mask = (offs_k[:, None] < K) & (offs_n[None, :] < N)
        
        a = tl.load(a_base + offs_k[None, :] * stride_ak, mask=a_mask, other=0.0)
        b = tl.load(b_base + offs_k[:, None] * stride_bk, mask=b_mask, other=0.0)
        
        acc = tl.dot(a, b, acc)
        offs_k += BLOCK_K
    
    offs_cm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_cn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    c_base = C_ptr + group_id * stride_cg
    c_ptrs = c_base + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
    
    if SPLIT_K == 1:
        tl.store(c_ptrs, acc.to(tl.bfloat16), mask=c_mask)
    else:
        tl.atomic_add(c_ptrs, acc, c_mask)

def grouped_gemm_naive(A, B):
    """Naive PyTorch implementation using a for loop"""
    G, M, K = A.shape
    _, K_, N = B.shape
    assert K == K_
    
    C = torch.empty((G, M, N), device=A.device, dtype=A.dtype)
    for g in range(G):
        C[g] = torch.matmul(A[g], B[g])
    return C

def grouped_gemm(A, B):
    G, M, K = A.shape
    _, K_, N = B.shape
    assert K == K_
    
    # Custom configuration for each size
    if M == 512:
        # Small - use extreme split-K
        C = torch.zeros((G, M, N), device=A.device, dtype=torch.float32)
        grid = lambda META: (
            G * triton.cdiv(M, META['BLOCK_M']) * triton.cdiv(N, META['BLOCK_N']) * META['SPLIT_K'],
        )
        grouped_gemm_kernel_optimized[grid](
            A, B, C,
            M, N, K, G,
            A.stride(0), A.stride(1), A.stride(2),
            B.stride(0), B.stride(1), B.stride(2),
            C.stride(0), C.stride(1), C.stride(2),
        )
        C = C.to(A.dtype)
    elif M == 2048:
        # Medium - use heavy split-K
        C = torch.zeros((G, M, N), device=A.device, dtype=torch.float32)
        grid = lambda META: (
            G * triton.cdiv(M, META['BLOCK_M']) * triton.cdiv(N, META['BLOCK_N']) * META['SPLIT_K'],
        )
        grouped_gemm_kernel_optimized[grid](
            A, B, C,
            M, N, K, G,
            A.stride(0), A.stride(1), A.stride(2),
            B.stride(0), B.stride(1), B.stride(2),
            C.stride(0), C.stride(1), C.stride(2),
        )
        C = C.to(A.dtype)
    elif M == 8192 and G == 1:
        # Single 8k - maximum split-K
        C = torch.zeros((G, M, N), device=A.device, dtype=torch.float32)
        grid = lambda META: (
            G * triton.cdiv(M, META['BLOCK_M']) * triton.cdiv(N, META['BLOCK_N']) * META['SPLIT_K'],
        )
        grouped_gemm_kernel_optimized[grid](
            A, B, C,
            M, N, K, G,
            A.stride(0), A.stride(1), A.stride(2),
            B.stride(0), B.stride(1), B.stride(2),
            C.stride(0), C.stride(1), C.stride(2),
        )
        C = C.to(A.dtype)
    elif M == 8192 and G == 4:
        # Multi-group 8k - moderate split-K
        C = torch.zeros((G, M, N), device=A.device, dtype=torch.float32)
        grid = lambda META: (
            G * triton.cdiv(M, META['BLOCK_M']) * triton.cdiv(N, META['BLOCK_N']) * META['SPLIT_K'],
        )
        grouped_gemm_kernel_optimized[grid](
            A, B, C,
            M, N, K, G,
            A.stride(0), A.stride(1), A.stride(2),
            B.stride(0), B.stride(1), B.stride(2),
            C.stride(0), C.stride(1), C.stride(2),
        )
        C = C.to(A.dtype)
    else:
        # Large - standard approach
        C = torch.empty((G, M, N), device=A.device, dtype=A.dtype)
        grid = lambda META: (
            G * triton.cdiv(M, META['BLOCK_M']) * triton.cdiv(N, META['BLOCK_N']) * META['SPLIT_K'],
        )
        grouped_gemm_kernel_optimized[grid](
            A, B, C,
            M, N, K, G,
            A.stride(0), A.stride(1), A.stride(2),
            B.stride(0), B.stride(1), B.stride(2),
            C.stride(0), C.stride(1), C.stride(2),
        )
    
    return C

def run_benchmarks():
    print_device_info()
    
    configs = [
        # G, M, K, N  ->  G=groups (like microbatches/layers), M=tokens, K=hidden, N=num_experts
        (1, 512, 896, 16),      # small batch
        (1, 2048, 896, 16),     # medium batch
        (1, 8192, 896, 16),     # 8k tokens (typical 8Ã—1024)
        (4, 8192, 896, 16),     # multiple layers/groups
        (8, 16384, 896, 16),    # large batch
    ]
    
    for G, M, K, N in configs:
        A = torch.randn(G, M, K, device='cuda', dtype=torch.bfloat16)
        B = torch.randn(G, K, N, device='cuda', dtype=torch.bfloat16)
        
        # Warm-up JIT and cache
        grouped_gemm(A, B)
        torch.bmm(A, B)
        grouped_gemm_naive(A, B)
        
        triton_ms = benchmark(grouped_gemm, A, B, warmup=50, rep=200)
        torch_ms = benchmark(torch.bmm, A, B, warmup=50, rep=200)
        naive_ms = benchmark(grouped_gemm_naive, A, B, warmup=50, rep=200)
        
        gflops = 2 * G * M * N * K / 1e9
        print(f"G={G}, shape=({M},{K},{N})")
        print(f" Triton: {triton_ms:.3f} ms, {gflops / (triton_ms / 1e3):.2f} GFLOP/s")
        print(f" Torch : {torch_ms:.3f} ms, {gflops / (torch_ms / 1e3):.2f} GFLOP/s")
        print(f" Naive : {naive_ms:.3f} ms, {gflops / (naive_ms / 1e3):.2f} GFLOP/s")
        print(f" Triton vs Torch: {torch_ms/triton_ms:.2f}x")
        print(f" Triton vs Naive: {naive_ms/triton_ms:.2f}x")
        print(f" Torch vs Naive: {naive_ms/torch_ms:.2f}x\n")

if __name__ == "__main__":
    run_benchmarks()